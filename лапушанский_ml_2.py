# -*- coding: utf-8 -*-
"""Лапушанский_ML_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vumdac5vSivGm9Arpezk4MrmlntOr-Mq

**Демо-экзамен по курсу "Машинное обучение"**

Выполнил: *Лапушанский Илья Вячеславович, М23-117*

Филиал: *Национальный
Исследовательский
Ядерный университет «МИФИ» (Московская площадка)*

---
# 1. Исходные данные

### 1.1 Загрузка библиотек и данных

*Загрузить данные в соответствии с вариантом задания*
"""

# Load data

from sklearn.datasets import load_diabetes
data = load_diabetes()

# Import libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as sp

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

"""### 1.2 Описание исходных данных

*Привести описание исходных данных, описание и типы признаков (вещественные, целочисленные, категориальные и т.д.), объём выборки, особенности данных. Сформулировать решаемую задачу, определить тип задачи (регрессия / классификация), указать входные и выходные переменные.*
"""

print(data.DESCR)

"""Датасет load_diabetes является стандартным набором данных в библиотеке scikit-learn. Этот набор данных содержит информацию о пациентах с диабетом и используется для предсказания количественного показателя прогрессирования болезни через год после измерений.

Датасет включает 10 признаков:

1. age - возраст пациента (целочисленный)
2. sex - пол пациента (категориальный)
3. bmi - индекс массы тела (вещественный)
4. bp - среднее артериальное давление (вещественный)
5. s1 - уровень холестерина в крови (целочисленный)
6. s2 - уровень сахара в крови (вещественный)
7. s3 - показатель инсулина (вещественный)
8. s4 - показатель массы печени (вещественный)
9. s5 - уровень сывороточного уровня (вещественный)
10. s6 - уровень сахара после еды (целочисленный )

Объём выборки составляет - 442.

Входными переменными выступают 10 признаков, описанных выше. Выходными - признак, который представляет собой количественный показатель прогрессирования диабета через год после прошлого измерения.

Таким образом, задача заключается в предсказании значения переменной на основе 10 входных признаков, из чего задача сводится к регресионной.

Приведем данные в нормированном и ненормированном состоянии
"""

df = pd.DataFrame(data.data, columns=data.feature_names)

df

data_2 = load_diabetes(scaled = False)

df_2 = pd.DataFrame(data_2.data, columns=data_2.feature_names)

df_2

"""Для упрощения построения модели машинного обучения в задаче мы в дальнейшем будем использовать стандартизированный датасет.

Перед началом анализа добавим в ДатаФрейм регресивный признак
"""

df

df['target'] = data.target

df

"""### 1.3 Выборочные характеристики

*Рассчитать основные выборочные характеристики (среднее, дисперсию, среднеквадратическое отклонение, медиану и т.д.), привести объемы выборок в каждом классе (для задач классификации)*
"""

df.describe()

"""### 1.4 Исследование распределений признаков и откликов

*Построить гистограммы распределения и диаграммы Box-and-Whisker (для отдельных признаков при большом их числе), сделать выводы о характере распределений признаков (для задач классификации - в классах), наличии выбросов и т.п.*

Выведем гистограммы распределения для каждого признака
"""

plt.figure(figsize=(15, 20))
for i, column in enumerate(df.columns, 1):
  plt.subplot(11, 2, 2 * i - 1)
  sns.histplot(df[column], kde=True)
  plt.title(f'Гистограмма признака {column}')
plt.tight_layout()
plt.show()

"""Большинство признаков имеют распределение, близкое к нормальному, что соответствует их нормированному виду.

Теперь рассмотрим Box-and-Whisker диаграммы для каждого признака
"""

plt.figure(figsize=(15, 20))
for i, column in enumerate(df.columns, 1):
  plt.subplot(11, 2, 2 * i - 1)
  sns.boxplot(x=df[column])
  plt.title(f'Box-and-Whisker гистограмма признака {column}')
plt.tight_layout()
plt.show()

"""Некоторые признаки, такие как s1 и s2, могут иметь выбросы, что видно по отдельным точкам за пределами "усов" на диаграммах, однако большинство признаков имеют умеренные или незначительные выбросы, что свидетельствует о том, что данные достаточно чистые.

### 1.5 Корреляционный анализ данных

*Визуализировать диаграммы рассеяния и корреляционную матрицу признаков, сделать выводы*

Визуализируем диаграммы рассеяния между всеми комбинациями признаков
"""

sns.pairplot(df)
plt.suptitle("Диаграммы рассеяния для признаков", y=1.02)
plt.show()

"""Также рассмотрим корреляционную матрицу признаков"""

matrix = df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.title("Корреляционная матрица признаков")
plt.show()

"""Большинство корреляций между признаками слабые. Это может  усложнить задачу для некоторых моделей машинного обучения.

### 1.6 Выводы

*Сделать выводы по результатам предварительного визуального анализа исходных данных*

Визуальный анализ показал, что данные относительно чисты, но могут потребовать минимальной обработки, такой как устранение выбросов.

Низкие корреляции между признаками предполагают, что для успешного моделирования могут потребоваться методы, способные улавливать сложные, нелинейные зависимости, такие как нелинейные модели (например, деревья решений)

На данном этапе не выявлено точных зависимостей между входными признаками и выходным параметром.

---
# 2. Предобработка данных

### 2.1 Очистка данных

*а) Обнаружение и устранение дубликатов*\
*б) Обнаружение и устранение выбросов*\
*в) Устранение/восстановление пропущенных значений*
"""

duplicates = df.duplicated().sum()
df = df.drop_duplicates()

print(f"Количество дубликатов: {duplicates}")

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

df_no_outliers = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
print(f"Количество выбросов, устраненных по методу IQR: {len(df) - len(df_no_outliers)}")
df = df_no_outliers

missing_values = df_no_outliers.isnull().sum()

print("Количество пропущенных значений в каждом столбце после очистки:")
print(missing_values)

df

"""### 2.2 Разбиение данных на обучающую и тестовую выборки

*Разбить данные на обучающую и тестовую выборки в отношении 70/30*
"""

X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f'Размер обучающей выборки: {X_train.shape[0]} образцов')
print(f'Размер тестовой выборки: {X_test.shape[0]} образцов')

"""### 2.3 Преобразование данных

*Описать используемые способы преобразования входных и выходных переменных, привести обоснования выбранных способов преобразования, применить преобразования к обучающей и тестовой выборкам*

Для успешного моделирования и повышения производительности моделей необходимо правильно обработать входные и выходные переменные. Одним из наиболее распрастроненных способов является стандартизация, то есть преобразование данных таким образом, чтобы признаки имели нулевое среднее значение и единичное стандартное отклонение. Стандартизация ускоряет процесс сходимости алгоритмов оптимизации и улучшает качество модели. Однако данные из библиотеки Sklearn.datasets уже были стандартизированны. На данном этапе у нас нет необходимости проводить дополнительные преобразования.

---
# 3. Построение и исследование модели машинного обучения

### 3.1 Обучение модели

*Выбрать модель и алгоритм машинного обучения для решения поставленной задачи, привести обоснование выбора, обучить модель на обучающей выборке*

Наша цель - прогнозирование прогрессирования диабета на основе медицинских показателей пациентов. Это задача регрессии, так как целевая переменная (target) является числовой и непрерывной. Для первого этапа обучения можно использовать модель "Ridge Regression", так как простая и эффективная модель может подойти для начального анализа перед использованием более сложных моделей, но вероятно даст более качественный результат чем Линейная Регрессия.
"""

ridge_model = Ridge(alpha=1.0)

ridge_model.fit(X_train, y_train)

y_train_pred = ridge_model.predict(X_train)

y_test_pred = ridge_model.predict(X_test)

"""### 3.2 Оценка качества модели

**Для задач регрессии:**
* *построить диаграммы рассеяния в пространстве «выход модели – желаемый выход» на данных обучающей и тестовой выборок*
* *построить линейные регрессии выхода модели на желаемый выход*
* *рассчитать коэффициенты детерминации линейных регрессионных моделей для обучающей и тестовой выборок*
* *построить гистограммы распределения ошибок модели.*

**Для задач классификации:**
* *построить матрицы ошибок (confusion matrix) классификатора и рассчитать показатели качества классификации (чувствительность, специфичность, точность, F-мера, каппа Коэна) на обучающей и тестовой выборках.*
"""

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(x=y_train, y=y_train_pred)
plt.title('Обучающая выборка')
plt.xlabel('Фактическое значение')
plt.ylabel('Предсказанное значение')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)

plt.subplot(1, 2, 2)
sns.scatterplot(x=y_test, y=y_test_pred)

plt.title('Тестовая выборка')
plt.xlabel('Фактическое значение')
plt.ylabel('Предсказанное значение')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)

plt.tight_layout()
plt.show()

"""Расчитаем коэффициенты детерминации"""

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"Коэффициент детерминации (R^2) на обучающей выборке: {r2_train}")
print(f"Коэффициент детерминации (R^2) на тестовой выборке: {r2_test}")

"""Построим диаграммы ошибок"""

errors_train = y_train - y_train_pred
errors_test = y_test - y_test_pred

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(errors_train, bins=30, kde=True)
plt.title('Распределение ошибок (обучающая выборка)')
plt.xlabel('Ошибка')
plt.ylabel('Частота')

plt.subplot(1, 2, 2)
sns.histplot(errors_test, bins=30, kde=True)
plt.title('Распределение ошибок (тестовая выборка)')
plt.xlabel('Ошибка')
plt.ylabel('Частота')

plt.tight_layout()
plt.show()

"""Можно увидеть, что и диаграммы рассеяния и диаграммы ошибок модели, показывают, что моделирование достаточно сильно расходится с реальными данными. Коэффициенты детерминации также дают понять, что модель на данном этапе не справляется с заданной задачей.

### 3.3 Исследование модели и алгоритма обучения

*Провести экспериментальные исследования модели, построить графики зависимости ошибки модели от ее архитектурных параметров и гиперпараметров алгоритма обучения, построить ROC-кривые, оценить степень важности признаков и пр.*
"""

alphas = np.logspace(-4, 4, 100)
train_errors = []
test_errors = []

for alpha in alphas:
    ridge_model = Ridge(alpha=alpha)
    ridge_model.fit(X_train, y_train)

    y_train_pred = ridge_model.predict(X_train)
    y_test_pred = ridge_model.predict(X_test)


    train_errors.append(mean_squared_error(y_train, y_train_pred))
    test_errors.append(mean_squared_error(y_test, y_test_pred))


plt.figure(figsize=(12, 6))

plt.plot(alphas, train_errors, label='Ошибка на обучающей выборке', color='blue')
plt.plot(alphas, test_errors, label='Ошибка на тестовой выборке', color='red')
plt.xscale('log')
plt.xlabel('Параметр alpha')
plt.ylabel('Среднеквадратическая ошибка (MSE)')
plt.title('Зависимость ошибки от параметра alpha')
plt.legend()
plt.show()

"""Обучение модели с лучший параметром alpha"""

best_alpha = alphas[np.argmin(test_errors)]

print (f'Лучший гиперпараметр alpha: {best_alpha}')

ridge_model = Ridge(alpha=best_alpha)
ridge_model.fit(X_train, y_train)

y_train_pred = ridge_model.predict(X_train)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)

print(f'Коэффициент детерминации (R^2) на обучающей выборке: {r2_train:.2f}')

"""Альтернативный метод поиска лучшего alpha используя метод GridSearchCV"""

param_grid = {'alpha': np.logspace(-4, 4, 100)}
ridge = Ridge()
grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_alpha = grid_search.best_params_['alpha']
best_model = grid_search.best_estimator_

print(f'Лучший гиперпараметр alpha: {best_alpha}')

ridge_model = Ridge(alpha=best_alpha)
ridge_model.fit(X_train, y_train)

y_train_pred = ridge_model.predict(X_train)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)

print(f'Коэффициент детерминации (R^2) на обучающей выборке: {r2_train:.2f}')

"""Видно, что при использовании оптимального параметра модель "Ridge Regression" способна дать более качественное предсказание.

### 3.4 Улучшение решения

*Предложить возможное улучшение точности решения задачи (выбрать другой тип модели, алгоритм или критерий обучения, сформулировать рекомендации по возможным способам повышения точности модели), обучить модель и сравнить показатели точности с рассчитанными в п.3.2*

На втором этапе анализа с помощью методов машинного обучения уже можно применить модель которая лучше подходит для данных с плохой корреляцией между признаками и способной к построению сложных нелинейных зависимостей. Такой моделью может выступить "Random Forest Regressor".
"""

model = RandomForestRegressor(random_state=42, n_estimators=100)
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

"""Проведем оценку качества модели приведенными выше способами. Построим диаграмму рассеяния"""

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(x=y_train, y=y_train_pred)
plt.title('Обучающая выборка')
plt.xlabel('Фактическое значение')
plt.ylabel('Предсказанное значение')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)

plt.subplot(1, 2, 2)
sns.scatterplot(x=y_test, y=y_test_pred)

plt.title('Тестовая выборка')
plt.xlabel('Фактическое значение')
plt.ylabel('Предсказанное значение')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)

plt.tight_layout()
plt.show()

"""Расчет коэффициентов детерминации"""

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"Коэффициент детерминации (R^2) на обучающей выборке: {r2_train}")
print(f"Коэффициент детерминации (R^2) на тестовой выборке: {r2_test}")

"""Построение гистограмм распределения ошибок модели"""

errors_train = y_train - y_train_pred
errors_test = y_test - y_test_pred

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(errors_train, bins=30, kde=True)
plt.title('Распределение ошибок (обучающая выборка)')
plt.xlabel('Ошибка')
plt.ylabel('Частота')

plt.subplot(1, 2, 2)
sns.histplot(errors_test, bins=30, kde=True)
plt.title('Распределение ошибок (тестовая выборка)')
plt.xlabel('Ошибка')
plt.ylabel('Частота')

plt.tight_layout()
plt.show()

"""Можно увидеть, что модель "Random Forest" действительно лучше справляется с прогнозирование результатов для данного датасета.

Теперь получим оптимальные параметры модели. Изучим зависимость 4 основных параметров "Random Forest": n_estimators( количество деревьев в лесу), max_depth(максимальная глубина дерева), min_samples_split(минимальное количество выборок, необходимое для разделения узла), min_samples_leaf(минимальное количество выборок, необходимое для образования листа) от ошибки модели.
"""

plt.figure(figsize=(14, 4))

n_estimators_range = range(10, 210, 20)
mse_estimators = []

for n in n_estimators_range:
    model = RandomForestRegressor(n_estimators=n, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_estimators.append(mse)

plt.figure(figsize=(14, 4))

plt.plot(n_estimators_range, mse_estimators, marker='o')
plt.title('Зависимости ошибки от параметра n_estimators')
plt.xlabel('n_estimators')
plt.ylabel('Среднеквадратическая ошибка (MSE)')

plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 4))

max_depth_range = range(1, 21, 2)
mse_depth = []

for depth in max_depth_range:
    model = RandomForestRegressor(n_estimators=100, max_depth=depth, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_depth.append(mse)


plt.plot(max_depth_range, mse_depth, marker='o')
plt.title('Зависимости ошибки от параметра max_depth')
plt.xlabel('max_depth')
plt.ylabel('Среднеквадратическая ошибка (MSE)')

plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 4))

min_samples_split_range = range(2, 22, 2)
mse_split = []

for split in min_samples_split_range:
    model = RandomForestRegressor(n_estimators=100, min_samples_split=split, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_split.append(mse)



plt.plot(min_samples_split_range, mse_split, marker='o')
plt.title('Зависимости ошибки от параметра min_samples_split')
plt.xlabel('min_samples_split')
plt.ylabel('Среднеквадратическая ошибка (MSE)')

plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 4))

min_samples_leaf_range = range(1, 21, 2)
mse_leaf = []

for leaf in min_samples_leaf_range:
    model = RandomForestRegressor(n_estimators=100, min_samples_leaf=leaf, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_leaf.append(mse)


plt.plot(min_samples_leaf_range, mse_leaf, marker='o')
plt.title('Зависимости ошибки от параметра min_samples_leaf')
plt.xlabel('min_samples_leaf')
plt.ylabel('Среднеквадратическая ошибка (MSE)')

plt.tight_layout()
plt.show()

"""Используя метод GridSearchCV получим оптимальные параметры модели"""

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Лучшие параметры: ", best_params)

"""И обучим модель используя полученные параметры"""

best_rf = RandomForestRegressor(**best_params, random_state=42)
best_rf.fit(X_train, y_train)

y_train_pred = best_rf.predict(X_train)
y_test_pred = best_rf.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"R^2 (Train Set): {r2_train}")
print(f"R^2 (Test Set): {r2_test}")

"""### 3.5 Выводы

*Сделать выводы по результатам проведенных исследований*

В рамках работы над экзаменом был изучен датасет пациентов с диабетом. Были рассмотрены свойства признаков датасета и оценены корреляции между ними. Были построены две модели машинного обучения для решения задачи, предсказания прогрессирования диабета на основе медицинский данных пациента. В результате наиболее качественный результат дала модель "Random Forest Regressor", что было ожидаемо, с учетом низкой корреляции между признаками в датасете. Модель имеет достаточно низкий Коэффициент детерминации (R^2) с значением близким к <0.5>, что делает модель недостаточно точной для качественного прогнозирования. Вероятно низкая эффективность модели может быть связанно с малой выборкой данных, а также с тем, что параметр регресии в отличии от других датасетов, включающих пациентов с диабетом, является вещественной (а не бинарной), а также обладает достаточно большим разбросом по значениям, из-за чего при прогнозировании сильно возрастает ошибка.
"""